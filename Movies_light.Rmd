---
title: "Modeling and prediction for movies"
subtitle: "ML - Regression"
author: "Anna Loznevaia"
date: "`r Sys.Date()`"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 6
---

Below is a short version of the file. If you interested in the full version (additional model diagnostics, robust and elastic net models) please check my github page <https://github.com/Myoth13/Movies/blob/main/Movies_reg.html>

```{r}

knitr::opts_chunk$set(warning=FALSE, message=FALSE)
knitr::opts_chunk$set(rows.print=25)
```

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(ggpubr)
library(dplyr)
library(statsr)
library(GGally)
library(cowplot)
library(lmtest)
library(leaps)
library(caret)
library(car)
library(olsrr)
library(tseries)
library(effectsize)
```

### Load data

```{r load-data}
load("movies.Rdata")

#month as factors
movies <- movies %>% 
  mutate(
    thtr_rel_month=as.factor(thtr_rel_month),
    dvd_rel_month=as.factor(dvd_rel_month))

#strip some data for prediction
train_size <- floor(0.90*nrow(movies))
set.seed(666)

# randomly split data
picked <- sample(seq_len(nrow(movies)),size = train_size)
movies <- movies[picked,]
movies_test <- movies[-picked,]
```

------------------------------------------------------------------------

## Part 1: Data

-   **Independence** - The data set is 651 randomly sampled movies produced and released before 2016. I have a concern about dependence of critics and general public ratings on published earlier reviews and ratings. I would argue that they are not independent and may be not suitable for modeling by OLS and at least it is a substantial source of a bias.

-   **Type** **of research** - observational study - only correlations can be established

-   **Generalizability** - random sample was taken and number of observations are large, but here are considerations for generalizability:

    -   looks like the ratings are relevant to the USA ( and probably Canada) population only - to be sure need to check the audience of IMDb and Rotten Tomatoes web services.

    -   only three movie types - Documentary, Feature Film, TV Movie are featured

    -   released from 1970 to 2016

    -   rating NC-17 not included

    -   bias source related to how scores are calculated (see below in research question)

------------------------------------------------------------------------

## Part 2: Research question

### Popularity of the movie

I will build a model to predict the popularity of the movie among general audience and critics using OLS (ordinary least square regression) on the given dataset, as well as research on each predictor variable independently during EDA (if something interesting there to be found). Also I will try to explain popularity if model allows.

From <https://en.wikipedia.org/wiki/Rotten_Tomatoes#Influence>

"The power of Rotten Tomatoes and fast-breaking word of mouth will only get stronger. Many Millennials and even Gen X-ers now vet every purchase through the Internet, whether it's restaurants, video games, make-up, consumer electronics or movies. As they get older and comprise an even larger share of total moviegoers, this behavior is unlikely to change".

In [Part 4: Modeling] I will build a few models for predicting general audience rating and select the best based on the predicting performance.

I will also build model for the `critics_score` to compare it with the general public score models.

#### Response variables

-   `critics_score`: Critics score on Rotten Tomatoes

-   `audience_score`: Audience score on Rotten Tomatoes

-   `imdb_rating`: Rating on IMDB

-   `imdb_num_votes`: Number of votes on IMDB, will be used to weight `imdb_rating`

from <https://www.makeuseof.com/tag/best-movie-ratings-sites/>

> The biggest issue with Rotten Tomatoes is that it breaks down complex opinions into a **Yes** or **No** score. It scores a critic who thought the movie was decent but had some flaws (say, a 59 percent rating) the same as one who thought the movie was absolute garbage (a zero percent score).
>
> You'll notice this with the **Average Rating** under the score. Take Jumanji: Welcome to the Jungle as an example. Of the 232 critic reviews, 177 of them are positive. This gives the movie a score of 76 percent. However, the critics rated the movie an average of 6.2/10---quite a bit under the 76 percent displayed on the page.

So it raises a concern about bias in the `audience_score` and if it is pertinent to combine it with the `imdb_rating` for the final audience score. And they both will suffer from the response bias - it is more likely to leave a review if you have a really strong opinion on the subject - either positive or negative. Moreover after 2018 Rotten Tomatoes implemented a new user filtering system based on purchased tickets from a particular provider, which renders another layer of bias on top of it (not applicable for this dataset though, but it makes the model not useful for fresh data).

#### Candidates for regressors

+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Variable           | Description                                                                                                                                                                                          |
+====================+======================================================================================================================================================================================================+
| `title_type`       | Type of movie (Documentary, Feature Film, TV Movie)                                                                                                                                                  |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `genre`            | Genre of movie                                                                                                                                                                                       |
|                    |                                                                                                                                                                                                      |
|                    | "Action & Adventure", "Animation" , "Art House & International", "Comedy", "Documentary", "Drama", "Horror", "Musical & Performing Arts", "Mystery & Suspense", "Other", "Science Fiction & Fantasy" |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `runtime`          | Runtime of movie (in minutes)                                                                                                                                                                        |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `mpaa_rating`      | MPAA rating of the movie (G, PG, PG-13, R, Unrated)                                                                                                                                                  |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `thtr_rel_year`    | Year the movie is released in theaters                                                                                                                                                               |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `thtr_rel_month`   | Month the movie is released in theaters                                                                                                                                                              |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `dvd_rel_year`     | Year the movie is released on DVD                                                                                                                                                                    |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `dvd_rel_month`    | Month the movie is released on DVD                                                                                                                                                                   |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `imdb_num_votes`   | Number of votes on IMDB                                                                                                                                                                              |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `best_pic_nom`     | Whether or not the movie was nominated for a best picture Oscar (no, yes)                                                                                                                            |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `best_pic_win`     | Whether or not the movie won a best picture Oscar (no, yes)                                                                                                                                          |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `best_actor_win`   | Whether or not one of the main actors in the movie ever won an Oscar (no, yes) -- note that this is not necessarily whether the actor won an Oscar for their role in the given movie                 |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `best_actress_win` | Whether or not one of the main actresses in the movie ever won an Oscar (no, yes) -- not that this is not necessarily whether the actresses won an Oscar for their role in the given movie           |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `best_dir_win`     | Whether or not the director of the movie ever won an Oscar (no, yes) -- not that this is not necessarily whether the director won an Oscar for the given movie                                       |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| `top200_box`       | Whether or not the movie is in the Top 200 Box Office list on BoxOfficeMojo (no, yes)                                                                                                                |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

Variables excluded from the potential regressors:

+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| Variable          | Description                                                                                 | Reason                                       |
+===================+=============================================================================================+==============================================+
| `thtr_rel_day`    | Day of the month the movie is released in theaters                                          | Not enough observations                      |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `dvd_rel_day`     | Day of the month the movie is released on DVD                                               | Not enough observations                      |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `critics_rating`  | Categorical variable for critics rating on Rotten Tomatoes (Certified Fresh, Fresh, Rotten) | Collinear with `critics_score` (derivative)  |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `audience_rating` | Categorical variable for audience rating on Rotten Tomatoes (Spilled, Upright)              | Collinear with `audience_score (derivative)` |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `director`        | Director of the movie                                                                       | Not enough data for this type of analysis    |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `actor1`          | First main actor/actress in the abridged cast of the movie                                  | Not enough data for this type of analysis    |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `actor2`          | Second main actor/actress in the abridged cast of the movie                                 | Not enough data for this type of analysis    |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `actor3`          | Third main actor/actress in the abridged cast of the movie                                  | Not enough data for this type of analysis    |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `actor4`          | Fourth main actor/actress in the abridged cast of the movie                                 | Not enough data for this type of analysis    |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `actor5`          | Fifth main actor/actress in the abridged cast of the movie                                  | Not enough data for this type of analysis    |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `imdb_url`        | Link to IMDB page for the movie                                                             | Irrelevant information                       |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+
| `rt_url`          | Link to Rotten Tomatoes page for the movie                                                  | Irrelevant information                       |
+-------------------+---------------------------------------------------------------------------------------------+----------------------------------------------+

: Excluded variables

Each variable will be explored separately in EDA and decisions will be made accordingly.

------------------------------------------------------------------------

## Part 3: Exploratory data analysis

### Response variables

```{r all_scores_pairs}

ggpairs(movies,columns = c("critics_score","audience_score","imdb_rating"))
```

`critics_score` **-** we can see the critic's tendency to score lower than the general audience.

`audience_score` has a quite high correlation with `imdb_rating`

#### IMDB scores weighting

`Imdb_num_votes`

Small amount of voters could be important source of a bias

```{r imdb_num_votes_sum}
summary(movies$imdb_num_votes)
```

We can see that the range is very wide. Seems quite legitimate to weight scores with a very low number of voters according to the amount of information it contains.

Score for IMDb will be weighted according to formula

$$
W=\dfrac{R*v + C*m}{v+m}
$$

Where

$W$ = weighted rating

$R$ = average for the movie as a number from 1 to 10 (mean) = (Rating)

$v$ = number of votes for the movie = (votes)

$m$ = minimum votes required to be considered significant - I've chosen 1st percentile (best guess for now)

$C$ = the mean vote across the whole report (sample mean 6.49)

Graphic of adjusted weights `imdb_rating_W` vs `imdb_rating`

```{r imdb_rating_w}

m <- quantile(movies$imdb_num_votes, probs = 0.01)

movies <- movies %>% 
  mutate(imdb_rating_W = (imdb_rating*imdb_num_votes + mean(imdb_rating)*m[[1]]) /
           (imdb_num_votes+m[[1]]) )

movies_test <- movies_test %>% 
  mutate(imdb_rating_W = (imdb_rating*imdb_num_votes + mean(imdb_rating)*m[[1]]) /
           (imdb_num_votes+m[[1]]) )

movies %>% 
  ggplot()+
    aes(x=imdb_rating, y=imdb_rating_W) +
    labs(x="Original rating",y="Scaled rating") +
    geom_hline(yintercept = mean(movies$imdb_rating), color="blue") +
    geom_point()
```

We can see that points with the lower number of voters moved towards the average proportionally to the information contained in it (`Imdb_num_votes`).

#### Final audience score

Final score will be normalized combination of `audience_score` and `imdb_rating`

```{r fin_score_dens, fig.height=3}

movies <- movies %>% 
  mutate(fin_score = (scale(audience_score)[,1] + scale(imdb_rating_W)[,1])/2) 

movies_test <- movies_test %>% 
  mutate(fin_score = (scale(audience_score)[,1] + scale(imdb_rating_W)[,1])/2) 

plot(density(movies$fin_score,na.rm=TRUE))
```

Distribution of the final score is more symmetric than original scores.

#### Critics score

Normalizing for consistency

```{r critics_score_norm}

movies <- movies %>% 
  mutate(critics_score = (scale(critics_score)[,1])) 

movies_test <- movies_test %>% 
  mutate(critics_score = (scale(critics_score)[,1])) 

```

### Regressors

#### `title_type`

```{r title_type_count}

movies %>% 
  count(title_type)
```

We can see that "TV Movie" type is underrepresented which could be a **source of a bias**

`fin_score` vs `title_type`

```{r title_type_fin_score}

ggpairs(movies,columns = c("critics_score","fin_score","title_type"))
```

#### `genre`

Distribution

```{r genre_dist}

movies %>% 
  ggplot(aes(x=forcats::fct_infreq(genre),fill = genre)) +
    geom_bar(show.legend=FALSE) +
    coord_flip()

```

We can see that some genres are underrepresented which could be a **source of a bias**

`genre` vs `fin_score`

```{r genre_fin_score}

movies %>% 
  ggplot() +
    aes(x = reorder(genre, fin_score, FUN = median), y = fin_score) +
    theme_light() +
    theme(axis.title.y=element_blank()) +
    stat_compare_means(size=4, label.x = 11, label.y = -3 ) +
    stat_mean(aes(color=genre)) + 
    stat_conf_ellipse (aes(color=genre)) +
    geom_boxplot(alpha=0) + 
    coord_flip() +
    theme(legend.position = "none")
```

We can see that the median `fin_score` is different among genres and some differences in the means are significantly.

`genre` vs `critics_score`

```{r genre_critic_score}

movies %>% 
  ggplot() +
    aes(x = reorder(genre, critics_score, FUN = median), y = critics_score) +
    theme_light() +
    theme(axis.title.y=element_blank()) +
    stat_compare_means(size=4, label.x = 11, label.y = -1 ) +
    stat_mean(aes(color=genre)) + 
    stat_conf_ellipse (aes(color=genre)) +
    geom_boxplot(alpha=0) + 
    coord_flip() +
    theme(legend.position = "none")
```

We can see that critics and general audience differs in magnitude and ranking

#### `runtime`

`runtime` vs `fin_score`

```{r runtime_fin_score}

ggpairs(movies,columns = c("critics_score","fin_score","runtime"))
```

Distribution of `runtime` is right-skewed with some outliers. There is a heteroscedasticity, I will apply log-modification in the model.

#### `mpaa_rating`

Distribution

```{r mpaa_rating, fig.height=3}

movies %>% 
  ggplot() +
    aes(x=forcats::fct_infreq(mpaa_rating),fill = mpaa_rating)+
    geom_bar(show.legend=FALSE) +
    theme(axis.title.y=element_blank()) +
    coord_flip()

```

We can see that only two movies with "NC-17" rating are represented in the sample, which could be a **source of a bias**

`mpaa_rating` vs `fin_score`

```{r mpaa_rating_fin_score, fig.height=3}

movies %>% 
  ggplot() +
    aes(x = reorder(mpaa_rating, fin_score, FUN = median), y = fin_score) +
    theme_light() +
    theme(axis.title.y=element_blank()) +
    stat_compare_means(size=4, label.x = 6, label.y = -2.5) +  #,  ) +
    stat_mean(aes(color=mpaa_rating)) + 
    stat_conf_ellipse (aes(color=mpaa_rating)) +
    geom_boxplot(alpha=0) + 
    coord_flip() +
    theme(legend.position = "none")
```

We can see a substantial difference in the some means and medians among ratings

#### Release dates

`thtr_rel_year` vs `dvd_rel_year` vs `fin_score` vs`critics_score`

```{r thtr_year_fin_score}

ggpairs(movies,columns = c("fin_score","critics_score","thtr_rel_year","dvd_rel_year"))
```

We can see a weak negative correlation between year and scores

`thtr_rel_month` vs `dvd_rel_month` vs `fin_score` vs`critics_score`

```{r thtr_month_fin_score}

ggpairs(movies,columns = c("fin_score","critics_score","thtr_rel_month","dvd_rel_month"))
```

We can see that distribution of months is pretty fair and there are some difference in the medians of the scores

#### `imdb_num_votes`

`imdb_num_votes` vs `fin_score`

```{r imdb_num_fin_score}

ggpairs(movies,columns = c("imdb_num_votes","fin_score","critics_score"))
```

Distribution of `imdb_num_votes` is heavily right-skewed with some outliers.

Heteroscedasticity, will apply log-transformation in the model

#### `best_pic_nom` and `best_pic_win`

`best_pic_nom` vs `fin_score`

```{r best_pic_nom_fin_score}

ggpairs(movies, columns = c("fin_score","critics_score","best_pic_nom"))
    
```

We can see that the medians differ substantially, but the number of nominated movies is small.

```{r best_pic_win_sum}

movies %>% 
  count(best_pic_win)
```

`best_pic_win` summary shows that there are too small amount of information available (it is duplicated with `best_pic_nom` anyway)

#### `best_actor_win` and `best_actress_win`

I will create a new indicator variable which include if any one of them win

```{r best_actor}

movies <- movies %>% 
  mutate(
    best_ac_win = if_else(best_actor_win=="yes" | best_actress_win=="yes","yes","no"))

movies_test <- movies_test %>% 
  mutate(
    best_ac_win = if_else(best_actor_win=="yes" | best_actress_win=="yes","yes","no"))
```

`best_actor` vs `fin_score`

```{r best_actor_fin_score}

ggpairs(movies, columns = c("fin_score","critics_score", "best_ac_win")) 
```

I do not see any substantial difference in the medians here.

#### `best_dir_win`

`best_dir_win` vs `fin_score`

```{r best_dir_fin_score}

ggpairs(movies, columns = c("fin_score","critics_score", "best_dir_win")) 
```

We can see a difference in the means, but there are a small number of counts in the "Yes" category.

#### `top200_box`

`top200_box` vs `fin_score`

```{r top200_fin_score}

ggpairs(movies, columns = c("fin_score","critics_score", "top200_box"))
```

We can see a difference in the medians, but there are too small counts in the "Yes" category.

#### `movies_short`

I will create a short version of the dataset containing only usable variables with all transformations.

```{r movies_short}

movies_short <- movies %>% 
  select (
    fin_score,
    critics_score,
    title_type,
    genre,
    runtime ,
    mpaa_rating ,
    thtr_rel_year ,
    thtr_rel_month,
    dvd_rel_year, 
    dvd_rel_month,
    imdb_num_votes ,
    best_pic_nom ,
    best_pic_win , 
    best_ac_win , 
    best_dir_win, 
    top200_box ) %>% 
  mutate(runtime = log(runtime), imdb_num_votes = log(imdb_num_votes))

movies_test <- movies_test %>% 
  mutate(runtime = log(runtime), imdb_num_votes = log(imdb_num_votes))

```

------------------------------------------------------------------------

## Part 4: Modeling

#### Full model

```{r full_model_sum}

model.full <- lm(fin_score ~ 
                  title_type +
                  genre +
                  runtime +
                  mpaa_rating +
                  thtr_rel_year +
                  thtr_rel_month +
                  dvd_rel_year +
                  dvd_rel_month +
                  imdb_num_votes +
                  best_pic_nom +
                  best_pic_win +
                  best_ac_win +
                  best_dir_win +
                  top200_box,
                data = movies_short
                )

summary(model.full)
```

#### Backward selection

I've chosen R^2^ based selection for maximizing predicting power from two options available - R^2^ based and p-value based (course limitation). So explanatory ability in this context will suffer.

Based on the full model adjusted R^2^ = 0.4606, I eliminated one predictor at a time

```{r backw_select_fin}

model.hm <- lm(fin_score ~ 
                  title_type +
                  genre +
                  runtime +
                  mpaa_rating +
                  thtr_rel_year +
                  #thtr_rel_month +
                  #####dvd_rel_year + 
                  dvd_rel_month + 
                  imdb_num_votes +
                  best_pic_nom +
                  ####best_pic_win +
                  best_ac_win , 
                  ###best_dir_win,  
                  ##top200_box, 
                data = movies_short
                )

summary(model.hm)$adj.r.squared
```

In the first round variable `thtr_rel_month` will be removed - 0.463912

2nd round `top200_box` - 0.4648519

3rd round `best_dir_win` - 0.4656855

4th round `best_pic_win` - 0.4662339

5th round `dvd_rel_year` - 0.4664639

Final model with adjusted R^2^ - 0.4665

#### Diagnostic of the handmade model

```{r fin_model_diagn}
hist_pl_fin <- ggplot(data = model.hm, aes(x = .resid)) +
  geom_histogram(binwidth = 0.1) +
  xlab("Residuals")

qq_pl_fin <- ggplot(data = model.hm, aes(sample = .resid)) +
  stat_qq() + stat_qq_line()

plot_grid(
  hist_pl_fin, 
  qq_pl_fin, 
  labels = c('Distribution', 'QQ norm'), label_size = 12)
```

Distribution of residuals left - skewed with some outliers.

```{r fin_model_res_plots, fig.height=6}
par(mfrow = c(2,2))
plot(model.hm)
```

Let's be honest - there are some heteroscedasticity, quite a lot of outliers and leverage points (some of them can be influential) and suspected non-linear trend in the residuals (could be due to independence violation, more research is needed)

##### OLS assumptions

1.  $\epsilon$ is a random variable that does not depend on X - "perfect model"

    Residual plot shows a pattern?

2.  $E(\epsilon)=0$ (always true for a model with an intercept)

3.  All $\epsilon_i$ are independent of each other (they are uncorrelated for the population, but not for the sample)

    \[Residuals autocorrelation\] in the full version

4.  $\epsilon \sim N(0,\theta_\epsilon)$ Residuals and $y_i$ are normally distributed

    They are not perfectly normal, but will do OK.

5.  all $\epsilon_i$ have the same PDF - homoscedasticity

    There are some heteroscedasticity, but will do OK for now.

#### ML - Exhaustive search

Let's use some silicon power =) There is not enough data and computational power to include interaction terms here, but let's compare with the manual model without interacting terms.

```{r fin_score_mod_ML, fig.height=3}

leaps <- regsubsets(fin_score ~. , data=movies_short, nbest = 1, nvmax=30,  
                    method = "exhaustive", force.out = c("critics_score"))

#choosing best model based on adjr2 (all options are - rsq, cp, adjr2, bic, rss)
sum = summary(leaps)

plot(sum$adjr2,type="l",xlab = "Number of parameters", ylab = "Adjusted R^2")
points(  which.max(sum$adjr2),sum$adjr2[which.max(sum$adjr2)],col="green")
```

This plot shows the best model (max adjusted R squared) which was found by the algorithm. Below are coefficients of this best model

```{r ml_model_coef}

(res_leaps<-as.data.frame(coef(leaps,which.max(sum$adjr2))))
```

I will save this model as:

```{r model.leaps}

model.leaps <- lm(fin_score ~ 
                    title_type +
                    genre +
                    runtime +
                    mpaa_rating +
                    thtr_rel_year +
                    thtr_rel_month +
                    dvd_rel_month +
                    imdb_num_votes +
                    best_pic_nom +
                    best_ac_win ,   
                data = movies_short
                )

summary(model.leaps)$adj.r.squared
```

We can see that this model has lower adjusted $R^2$ than the handmade model and different predictors.

#### Model for the `critics_score`

```{r critics_score_model, fig.height=3}

leaps_cr <- regsubsets(critics_score ~. , data=movies_short, nbest = 1, nvmax=30,
                       method = "exhaustive", force.out = c("fin_score"))

sum_cr = summary(leaps_cr)

plot(sum_cr$adjr2,type="l",xlab = "Number of parameters", ylab = "Adjusted R^2")
points(which.max(sum_cr$adjr2),sum_cr$adjr2[which.max(sum_cr$adjr2)],col="green")
```

Coefficients for this model

```{r model_critics_score_coef}

(res_critics <- as.data.frame(coef(leaps_cr,which.max(sum_cr$adjr2))))
```

We can see that the model is slightly different. I will save this model as:

```{r model_critics_score}

model.crit = lm (critics_score ~
                    title_type +
                    genre +
                    mpaa_rating +
                    thtr_rel_year +
                    thtr_rel_month +
                    dvd_rel_year +
                    dvd_rel_month +
                    imdb_num_votes +
                    best_pic_nom +
                    best_dir_win ,
                  data = movies_short
                  )

summary(model.crit)$adj.r.squared
```

Model for the `critics_score` has lower adjusted R^2^ than the similar ML model for the `fin_score` and different regressors.

Critics rating is harder to predict than general audience's (and most likely to explain too).

#### Cross validation selection

```{r cross_valid_learning, message=FALSE, warning=FALSE}

# Set up repeated k-fold cross-validation
train.control <- trainControl(method = "cv", number = 5)

# Train the model
step.model <- train(fin_score ~.,
                    data = movies_short,
                    method = "leapSeq",
                    tuneGrid = data.frame(nvmax = 20:48),
                    trControl = train.control,
                    na.action = na.exclude,
                    force.out = c("critics_score")
                    )

#Best number of parameters
step.model$bestTune
```

Coefficients for this model

```{r model_cross_coef}

(res_caret <- as.data.frame(coef(step.model$finalModel, step.model$bestTune[[1]])))
```

Because search is not exhaustive it brings back different results each time it runs. I will save one of the models:

```{r caret_model}

model.caret <- lm(fin_score ~ 
                    title_type +
                    genre +
                    runtime +
                    mpaa_rating +
                    thtr_rel_year +
                    thtr_rel_month +
                    dvd_rel_year +
                    dvd_rel_month +
                    imdb_num_votes +
                    best_pic_nom +
                    best_pic_win ,   
                data = movies_short
                )

summary(model.caret)$adj.r.squared
```

#### Interaction terms

As I was reviewing one of the projects (<https://coursera-assessments.s3.amazonaws.com/assessments/1641179529192/4a75f31b-f9e2-4f9d-a698-98df195df83a/reg_model_project.html>) I took home the idea about `runtime` significance when only one `title_type` was selected. So I will add one more model which will include interaction term between them (based on the previous `model.caret` ).

```{r model_interact}
 
model.interac <- lm(fin_score ~ 
                    title_type +
                    genre +
                    runtime +
                    mpaa_rating +
                    thtr_rel_year +
                    thtr_rel_month +
                    dvd_rel_year +
                    dvd_rel_month +
                    imdb_num_votes +
                    best_pic_nom +
                    best_pic_win +   
                    title_type*runtime,   
                data = movies_short
                )

summary(model.interac)$adj.r.squared
```

Some promising results, will see its predictive power later

## Part 5: Prediction

Instead of one of the movie from 2016 I will use test part of the data. (at the beginning I split the original dataset in two - movies and movies_test. I used movies to train models. Now I will use the remain part to predict and compare my models)

#### Handmade model

Few points with their CI

```{r predict_CI}

head(predict(model.hm, movies_test, interval = "prediction", level = 0.95))
```

Lets' compare with their original scores

```{r predict_compare}

pred <- predict(model.hm, movies_test)
head(cbind(actual=movies_test$fin_score, predicted=pred))
```

We can see that the uncertainty in the prediction values is very high and precision is not great as well.

#### Comparing models

Some metrics which based on means of not squared parameters (Min-Max Accuracy and Mean Absolute Percentage Error (MAPE) for example) are stopped working after normalization of response variables

I will calculate two metrics: MSE and Effron's R squared

$$
MSE = \dfrac{1}{n}\sum_{i=1}^N(y_i-\hat{y}_i)^2
$$ $$
R^2= 1-\dfrac{\sum_{i=1}^N(y_i-\hat{y}_i)^2}{\sum_{i=1}^N(y_i-\bar{y})^2}
$$

##### Metrics on the test data

```{r model_compare}

#function to calculate metrics
calc_metrics <- function(model.name,newdata,response.var) {
  argg <- as.list(environment())
  pred <- predict(model.name,newdata = newdata)
  #pred <- predict(model.name,newx = newdata)
  act_pred <- data.frame(cbind(act=response.var, pred=pred))
    
  MSE <- mean((act_pred$act - act_pred$pred)^2,na.rm=T)
  efR <- 1 - sum( (act_pred$act - act_pred$pred)^2 ,na.rm=T)/
    sum( (act_pred$act-mean(act_pred$act,na.rm=T))^2 ,na.rm=T)
  
  df <- data.frame(deparse(substitute(model.name)),round(MSE,4),round(efR,4))
  names(df) <- c("Model","MSE","Efron'sR^2")
  return(df)
}

#metrics for all the models
res_table <- calc_metrics(model.full,movies_test,movies_test$fin_score)
res_table <- rbind(res_table,calc_metrics(model.hm,movies_test,movies_test$fin_score))
res_table <- rbind(res_table,calc_metrics(model.leaps,movies_test,movies_test$fin_score))
res_table <- rbind(res_table,calc_metrics(model.caret,movies_test,movies_test$fin_score))
res_table <- rbind(res_table,calc_metrics(model.interac,movies_test,movies_test$fin_score))
res_table <- rbind(res_table,calc_metrics(model.crit,movies_test,movies_test$critics_score))
res_table
```

#### Diagnostic of the best model

The best model seems to be model based on cross-validation selection technique with interaction terms. (Very important lesson to take away - know you data!)

Let's check if this model comply with OLS assumptions

```{r best_model_diagnostic, fig.height=6}

par(mfrow = c(2,2))
plot(model.interac)
```

There are the same problem as in the handmade model - heteroscedasticity, residual distribution is left-skewed, outliers, leverage points and non-linear trend in the residuals. Definitely more research is needed.

## Part 6: Conclusion

1\) Model effect size

For the explanation of the model I've chosen effect size over p-value, because if something is statistically significant it is not necessarily practically significant. In my opinion effect size shows more tangible connection with the real world.

Standardized model parameters (coefficients) - the standardized parameter is the difference between the means in SD of the response variable (`fin_score`).

As it is similar to a **Cohen's *d*** (Cohen's d is the distance between the means in units of [**pooled SDs**](https://easystats.github.io/effectsize/reference/sd_pooled.html).) I will use this rule of thump

**Cohen (1988)**

-   **d \< 0.2** - Very small

-   **0.2 \<= d \< 0.5** - Small

-   **0.5 \<= d \< 0.8** - Medium

-   **d >= 0.8** - Large

More about standardized model parameters <https://cran.r-project.org/web/packages/effectsize/vignettes/standardize_parameters.html>

Best model according to the test data is `model.interac` :

```{r best_model_effect}

effectsize(model.interac)
```

-   `title_type`

decreasing in 1 SD from its base level - "Documentary" for both "Feature Film" and "TV Movie"

-   `genre`

    significant influence for many levels

-   `mpaa_rating` of a movie have substantial influence on its popularity.

-   `thtr_rel_year`

older movies have a small tendency to be rated higher by the audience

-   `thtr_rel_month` and `dvd_rel_month`

Month of a release has some influence too, but it is not a very strong effect size.

-   `imdb_num_votes` and `best_pic_nom`

We can see that the number of voters and if a movie had been nominated are correlated with its popularity with medium effect size.

-   `title_type:runtime`

There is a very strong influence of the `runtime` variable (log of it if to be exact) on the "TV Movie" level of `title_type` But if you remember - there are only 5 movies this category - so it is probably just noise.

-   And finally enigmatic very small negative effect size of the `best_pic_win` most likely due to collinearities between this variable and `best_pic_nom`. If anyone knows the exact reason please let me know.

2\) Quality of the model.

Firstly - there is not enough information in the dataset - many predictor levels have too small numbers of observations. The best possible solution is to increase a sample size. Also methods like <https://www.researchgate.net/publication/257364616_SMOTE_for_Regression> can be useful if collecting more data is not an option.

Another possible issue is the non independence of observations - if you violate the assumption of independence, **you run the risk that all of your results will be wrong**, so more research is needed.

There are a few other minor problems: violation of normality, non-linearity, influential points, collinearity and heteroscedasticity. I've tried some options like robust regression and elastic net regularization to deal with it. I would like to try some ensemble methods later.
